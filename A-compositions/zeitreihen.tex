\section{Zeitreihen}
\label{sec:compositions:timeanalysis}
\subsection{Einführung} Diese Ausarbeitung wird einen Einstieg in das
Thema Zeitreihenverarbeitung geben.

In vielen Domänen ist die zeitliche Betrachtung von Daten von hoher
Bedeutung. Bei der Analyse von Aktienmärkten, DNA Sequenzen und
Sprache findet man zeitlich zusammenhängende Daten ( Zeitreihen ), die
nicht punktuell betrachtet werden können. Auch in Rechnernetzen trifft
man auf zeitlich zusammenhängende Daten wie Paketströme, Anzahl von
Anfragen ( gemessen über die Zeit ) oder Anzahl der TCP Pakete (
gemessen über die Zeit ).

Um eine Einführung in die Verarbeitung solcher Daten zu geben wird der
Begriff der Zeitreihe formal eingeführt und gegen punktuelle
Lernverfahren abgegrenzt. Grundlegende statistische Verfahren zur
Verarbeitung von Zeitreihen wie ``Sampling'', ``Sliding Window'',
``Sample Mean'' und ``Sample Variance'' werden erläutert.

Nachfolgend wird das Distanzmaß ``Dynamic Time Warping'' (DTW)
vorgestellt und die Möglichkeit Zeitreihen damit zu klassifizieren (
Nächste Nachbarn / SVM mit DTW Kernel ) grundlegend dargestellt. Zum
indizieren und finden werden die Zeitreihenrepräsentation ``Symbolic
Aggregate approXimation'' und ``indexable Symbolic Aggregate
approXimation'' (SAX / iSax) vorgestellt mit denen es möglich ist
große Mengen Zeitreihen zu durchsuchen.

\subsection{Zeitreihen und Statistik} In diesem Abschnitt sollen die
grundlegenden statistischen Verfahren auf Zeitreihen eingeführt werden
für eine genauere Einführung sei auf \cite{ts, gama} verwiesen.  Eine
Zeitreihe beschreibt eine Reihung realwertiger Zahlen im zeitlichen
Kontext definiert als (\cite{shieh08}):

\begin{align}
  X = x_1, x_2, \ldots,  x_t \text{ mit $x_i \in \mathbb{R}$ und der
    Zeitpunkt $t \in \mathbb{N}$ }
\end{align}

Um solch eine Zeitreihe zu erhalten wird oft \textbf{Sampling}
genutzt.  Sampling beschreibt periodisches Messen einer Variable.
Beispiele sind stündliches Messen der Temperatur oder das stündliche
Messen der Anzahl der UDP Pakete in einem Subnetz.

Da oft nicht alle vorherigen Ereignissen interessant sind, sondern nur
die in der nahen Vergangenheit, wird oft ein \textbf{Sliding Window}
genutzt. Solch ein Fenster ist ein Bereich fester Größe (mit
$length(window) << length(data)$) der durch den Datensatz elementweise
geschoben wird. Es wird folglich pro resultierendem Fenster
ausgewertet (siehe Beispiel~\ref{img:sw}).

\begin{figure} [ht]                                   
\centering
\includegraphics[width=0.5 \textwidth]{images/sliding_window}  
\caption{Eine Zeitreihe $\{1,2,3,4,5,6,7,8,1\}$der Länge 8 und ein
  Sliding Window der Länge 6.}         
\label{img:sw}
\end{figure}

Auf diesen Fenstern können nun ``lokale'' Statistiken geführt werden
(lokal da nur ein Ausschnitt betrachtet wird und nicht die gesamte
Zeit).  Zwei der häufigsten Statistiken sind der Mittelwert $\mu$
(eng. mean) und die Varianz $\sigma^2$(eng. variance). Der Mittelwert
über ein Fenster ist definiert als:

\begin{align}
\mu(X) = \frac{1}{N}\Sigma_{i = start} ^ {N} x_i \text{ mit } N = length(window)
\end{align}

Die Varianz gibt die Streuung oder Abweichung vom Mittelwert an und
ist definiert als:

\begin{align} \sigma^2(X) = \Sigma_{i = start} ^ {N} (x_i - \mu )^2
\end{align}

Für weiterführende Literatur empfehlen sich \cite{gama, ts}.

\subsection{Dynamic Time Warping Distanz}

Dynamic Time Warping (DTW) ist ein Distanzmaß für Zeitreihen ähnlich
der Levenstein oder Editierdistanz.  In Abbildung~\ref{img:dtw} a)
sind zwei beispielhafte Zeitreihen angegeben zu denen die DTW Distanz
gesucht ist.  Im ersten Schritt werden diese mit Dynamic Programming
justiert oder in Deckung gebracht (siehe~\ref{img:dtw} b) ).
Nachfolgend werden die justierten Zeitreihen so übereinander gelegt,
dass die Länge der Abweichungen (graue Linien in~\ref{img:dtw} c))
minimal ist. Die Distanz ergibt sich dann aus der Summe der
Abweichungen.


Die Varianz gibt die Streuung oder Abweichung vom Mittelwert an und
ist definiert als:

\begin{align} \sigma^2(X) = \Sigma_{i = start} ^ {N} (x_i - \mu )^2
\end{align}

Für weiterführende Literatur empfehlen sich \cite{gama, ts}.

\subsection{Dynamic Time Warping Distanz}

Dynamic Time Warping (DTW) ist ein Distanzmaß für Zeitreihen ähnlich
der Levenstein oder Editierdistanz.  In Abbildung~\ref{img:dtw} a)
sind zwei beispielhafte Zeitreihen angegeben zu denen die DTW Distanz
gesucht ist.  Im ersten Schritt werden diese mit Dynamic Programming
justiert oder in Deckung gebracht (siehe~\ref{img:dtw} b) ).
Nachfolgend werden die justierten Zeitreihen so übereinander gelegt,
dass die Länge der Abweichungen (graue Linien in~\ref{img:dtw} c))
minimal ist. Die Distanz ergibt sich dann aus der Summe der
Abweichungen.

\begin{figure}[ht]
  \centering
  \includegraphics[width= 0.5 \textwidth]{images/dtw}
  \caption{Berechnen der DTW Distanz, Übernommen mit Berechtigung von
    Daniel Ashbrook}
  \label{img:dtw}
\end{figure}

Formal definiert ist die Dynamic Time Warping Distanz wir folgt.  Für
zwei Zeitreihen $C = C_1, C_2, ..., C_n $ und $Q = Q_1, Q_2, ..., Q_m$
ist Dynamic Time Warping rekursiv definiert als \cite{wei08, keogh02}:
\begin{eqnarray} DTW([ ],[ ]) & = & 0 \\ DTW(C, [ ]) & = & \infty \\
DTW([ ], Q) & = & \infty \\ DTW(C,Q) & = & abs(First(C), First(Q)) \\
& & + min(\\ & & DTW(C, Rest(Q)) \text{ insertion} \\ & &
DTW(Rest(C),Q) \text{ deletion} \\ & & DTW(Rest(C), Rest(Q)) \text{
match}
\end{eqnarray}

Bildlich wird eine Distanzmatrix der Größe $n x m$ aufgebaut ( x-Achse
repräsentier $C$ und y-Achse $Q$) und der Pfad mit den minimalen
Kosten durch die Matrix gesucht (siehe~\ref{img:wp}).
\begin{figure}[ht] \centering
  \includegraphics[width= 0.5 \textwidth]{images/dtw2}
  \caption{Warping Path der DTW Distanz, Übernommen mit Berechtigung
von Eamonn Keogh\cite{keogh02}}
  \label{img:wp}
\end{figure}

\subsection{Klassifizierung mit Dynamic Time Warping}
\subsubsection{Nächster Nachbar} Nächster Nachbar (eng. Nearest
Neighbour) ist ein instanzbasiertes Verfahren zum Klassifizieren. Im
Gegensatz zu Klassifizierern wie Support Vector Machines oder naive
Bayes muss der Algorithmus nicht trainiert werden. Die
Trainingsbeispiele werden vorgehalten bis eine Instanz klassifiziert
werden soll. Für jede nicht klassifizierte Instanz gibt die
Klassifizierung des Trainingsbeispiels mit der minimalen Distanz zur
zu klassifizierenden Instanz die Klassifizierung an.  Für Zeitreihen
wird meist die Dynamic Time Warping Distanz genutzt.
\subsubsection{Support Vector Machines} % ref Eine generelle
Einführung in die Klassifizierung und das Trainieren von Support
Vector Machines befindet sich im ersten Halbjahresbericht des
Projektes \f. Grundlegend wird ein linearer Klassifikator im
sogenannten ``Kernel Space'' gelernt. Ein Kernel $K$ ist eine Funktion
die gegeben zweier Instanzen $x$ und $y$ so in $\mathbb{R}$ abbildet,
dass eine lineare Trennung möglich ist (Schreibweise: $K(x, y)$). Oft
verwendete Kernel sind der lineare Kernel $K(x,y) = x * y$ und der
polinomielle Kernel $K(x,y) = ( x * y) ^d$ (hier ist $*$ das
Scalarprodukt). Um Zeitreihen zu Lernen und Klassifizieren kann ein
Dynamic Time Warping Kernel verwendet werden. Seien hier $x$ und $y$
Zeitreihen (mit $length(x) = n$ und $length(y) = m$), dann ist der
Kernel $K(x,y) = DTW(x,y)$. Es wird also im Dynamic Time Warping Raum
gelernt \cite{svm_dtw}. Zu beachten sei, dass die SVM hier als eine
Art gewichteter instanzbasierter Klassifikator ist.

\subsection{Symbolic Aggregate Approximation} Symbolic Aggregate
approXimation (SAX) \cite{lin07} überführt eine Zeitreihe in eine
symbolische Approximation (einen String).  Da Suche oder Indizierung
von Strings eine gut verstandene Disziplin in Algorithmik und Bio-
Informatik sind, können bestehende Verfahren auf Zeitreihenanalyse
angewendet werden.  Die Transformation einer Zeitreihe in einen String
wird im Folgenden erläutert.

Eine Zeitreihe $X = x_1, x_2, ..., x_t$ wird zunächst mittels
Piecewise Aggregate Approximation (PAA) \cite{keogh00} auf eine
gewählte Länge $N$ kompremiert. Das Ergebnis ist die kompremierte
Zeitreihe $\bar{X} = \bar{x_1}, \bar{x_2}, ..., \bar{x_N}$ wobei für
$\bar{x_i}$ gilt:

\begin{align}
\bar{x_i} = \frac{t}{N} \Sigma_{j = t / N(i-1)+1}^{n/w*i} X_i
\end{align}

Folglich teilt die PAA die Zeitreihe $X$ in $N$ gleich große Blöcke
und speichert den Mittelwert pro Block.  Um die Zeitreihe in einen
String zu überführen wird zusätzlich zur Wortlänge eine Alphabetgröße
(genannt Kardinalität, eng cardinality) $\alpha = |\Sigma|$
eingeführt. $SAX(N, \alpha)$ teilt die Y-Achse in $\alpha$
Regionen. Jeder PAA - Wert wird durch das Symbol ersetzt, das mit der
Region assoziiert ist, in der dieser Wert fällt.  Diese Regionen sind
so gewählt, das die Wahrscheinlichkeit in jeder Region gleich ist.
Für gleich verteilte Zeitreihen ist dies $\frac{1}{\alpha}$ und für
normalverteilte Zeitreihen eine Region, sodass die Fläche unter einer
Gausskurve $\frac{1}{\alpha}$ ist. Die Regionen für Normalverteilungen
können in statistischen Tabellen nachgeschlagen werden.  Das Ergebnis
ist ein String $S = s_1,..., s_N$ mit $s_i \in \Sigma$.  Sei unsere
Zeitreihe beispielsweise: $X = [1,4,1,4,1,2,1,2,6,8]$ und unsere
Alphabetlänge $\alpha = 3$ (also $\Sigma = \{A,B,C\}$).  Die Anfänge
der Regionen seien $[1,2,7]$ (in diesem Beispiel weder Normal noch
Gaussverteilt). Die $PAA(5,X)$ ist dann $[2.5,2.5,1.5,1.5,7]$ und der
String ``BBAAC''.

Der gesamte Prozess ist in Abbildung~\ref{img:sax} abgebildet.
\begin{figure}[ht] \centering
\includegraphics[width= 0.8\textwidth]{images/sax_minnen}
\caption{Von der Zeitreihe zu SAX. Übernommen mit Berechtigung von
David Minnen}
\label{img:sax}
\end{figure}

\subsection{indexable Symbolic Aggregate Approximation} Um zu einer
Zeitreihe schnell nächste Nachbarn zu finden soll iSAX, als
Erweiterung zu SAX, vorgestellt werden.  Die Idee ist alle Zeitreihen
in SAX zu transformieren und den resultierenden String als Index zu
nutzen. Zeitreihen mit dem gleichen Index werden dann unter diesem
gemeinsam gehalten. Meist wird dies durch Dateien mit dem Index als
Namen und den Zeitreihen als Inhalt realisiert.  In iSax
\cite{shieh08, shiehPHD} wird zusätzlich zu den SAX Parametern noch
eine sogenannte ``bucket size'' angegeben. Diese gibt an wie viele
Zeitreihen pro Index gehalten werden können. Übersteigt die Anzahl der
Zeitreihen in einem Index diesen Schwellwert wird die Alphabetgröße an
einer Position des Strings (mittels Round Robin) erhöht, die Datei
durch einen Ordner mit gleichem Namen ersetzt und alle Zeitreihen
werden mit der höheren Alphabetgröße( in dem Ordner) neu indiziert.
Durch das erhöhen der Alphabetgröße an einer Position werden die
Zeitreihen besser unterscheidbar, da es nun an einer Position mehr
Symbole und daher schmalere Regionen gibt.

Aus diesem Verfahren ergibt sich dann ein Suchbaum~\ref{img:sax_tree}.
\begin{figure}[ht] \centering
\includegraphics[width= 0.8\textwidth]{images/isax_tree}
\caption{Ein iSAX Suchbaum }
\label{img:sax_tree}
\end{figure}

Um zu einer Zeitreihe nächste Nachbarn zu finden, wird diese in SAX
Form unter den Bedingungen des Wurzelknotens transformiert und dann
den Indizes rekursiv gefolgt, bis eine Enddatei erreicht ist. In
dieser befinden sich dann die Kandidaten.

\subsection{Zusammenfassung} In dieser Ausarbeitung wurden
grundlegenden Statistiken und Definitionenen von Zeitreihen
beschrieben.  Außerdem wurde Dynamic Time Warping als Distanzmaß und
Basis für Klassifikatoren eingeführt. Im letzten Abschnitt wurde eine
Transformation von Zeitreihen in Strings beschrieben mit der es
möglich ist Suchindizes für große Datensätze aufzubauen.

